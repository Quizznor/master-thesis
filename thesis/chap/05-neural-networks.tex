%! TEX root = ../thesis.tex

\chapter{Neural networks}
\label{chap:neural-networks}

The idea of a \textbf{N}eural \textbf{N}etwork (NN) is to attempt to capture the human thinking process in machine code. For this purpose, a network architecture
connects some input (e.g. a picture) to an output (e.g. digits 0-9). Much like in a human brain, the architecture consists of multiple smaller chunks, neurons 
and layers, which connect in some way to form an emergent intelligence. 

As described, neural network do not yet hold the abilities to achieve their designated tasks, and can hardly be called intelligent. They need to be trained. 
This is done by presenting an example input (called training data) to the network. The network output is compared to the desired output for the given input via 
some loss function. During training, the network attempts to minimize this loss function. How it is minimized is often a design choice, and in general will 
depend on the network architecture, which in turn is influenced by the type of data and kind of task the NN should accomplish.

In the following several network architectures which are relevant for this work are detailed. The most simple option of a \textbf{D}ense NN (DNN) is given in 
\autoref{sec:DNN} in order to introduce several key concepts. \textbf{C}onvolutional NNs (CNNs) used for image recognition are explained in \autoref{sec:CNN}. 
Lastly \textbf{R}ecurrent NNs (RNNs) that find an application in time series analysis are shown in \autoref{sec:RNN}

\section{Dense neural network}
\label{sec:DNN}

Dense neural networks are subdivided into layers, which themselve consist of individual neurons.


\section{Convolutional neural network}
\label{sec:CNN}

\section{Recurrent neural network}
\label{sec:RNN}